{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('../Train.csv')\n",
    "test_df = pd.read_csv('../Test.csv')\n",
    "\n",
    "# Preview the first few rows of each dataset\n",
    "train_preview = train_df.head()\n",
    "test_preview = test_df.head()\n",
    "\n",
    "train_preview, test_preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard the 'Yield' column from the train dataset\n",
    "train_df.drop('Yield', axis=1, inplace=True)\n",
    "\n",
    "# Combine the datasets into one\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined dataset to verify\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_values = combined_df.isnull().mean() * 100\n",
    "\n",
    "# Display the percentage of missing values for each column\n",
    "missing_values.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Function to impute missing dates based on the median date within the same district or block\n",
    "def impute_dates(dataframe, date_columns):\n",
    "    for column in date_columns:\n",
    "        # Convert to datetime\n",
    "        dataframe[column] = pd.to_datetime(dataframe[column], errors='coerce')\n",
    "        # Group by 'District' and 'Block' and then transform using the median date\n",
    "        dataframe[column] = dataframe.groupby(['District', 'Block'])[column].transform(lambda x: x.fillna(x.median()))\n",
    "    return dataframe\n",
    "\n",
    "# Categorical columns with low missingness (<15%)\n",
    "categorical_cols_low_miss = missing_values.index[(missing_values < 15) & (combined_df.dtypes == 'object')]\n",
    "\n",
    "# Continuous numerical columns with low missingness (<15%)\n",
    "numerical_cols_low_miss = missing_values.index[(missing_values < 15) & (combined_df.dtypes != 'object')]\n",
    "\n",
    "# Date columns\n",
    "date_columns = ['CropTillageDate', 'RcNursEstDate', 'Harv_date', 'Threshing_date']\n",
    "\n",
    "# Impute missing dates\n",
    "combined_df = impute_dates(combined_df, date_columns)\n",
    "\n",
    "# Simple Imputer for categorical data\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "combined_df[categorical_cols_low_miss] = mode_imputer.fit_transform(combined_df[categorical_cols_low_miss])\n",
    "\n",
    "# Simple Imputer for numerical data\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "combined_df[numerical_cols_low_miss] = mean_imputer.fit_transform(combined_df[numerical_cols_low_miss])\n",
    "\n",
    "# Check missing values again\n",
    "new_missing_values = combined_df.isnull().mean() * 100\n",
    "\n",
    "combined_df[date_columns].info(), combined_df[categorical_cols_low_miss].info(), combined_df[numerical_cols_low_miss].info(), new_missing_values.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Drop features with more than 70% missingness\n",
    "threshold = 70\n",
    "features_to_drop = missing_values.index[missing_values > threshold].tolist()\n",
    "combined_df.drop(columns=features_to_drop, inplace=True)\n",
    "\n",
    "# Impute numerical features with high missingness using k-NN\n",
    "# Identifying numerical columns with high missingness (between 15% to 70%)\n",
    "numerical_cols_high_miss = missing_values.index[(missing_values >= 15) & (missing_values <= 70) & (combined_df.dtypes != 'object')].tolist()\n",
    "\n",
    "# Since k-NN imputer works only with numerical data, ensure to exclude any non-numerical columns\n",
    "numerical_cols_for_knn = [col for col in numerical_cols_high_miss if combined_df[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# k-NN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "combined_df[numerical_cols_for_knn] = knn_imputer.fit_transform(combined_df[numerical_cols_for_knn])\n",
    "\n",
    "# For categorical variables, we impute missing values with the most frequent value within the same district or block\n",
    "categorical_cols_high_miss = missing_values.index[(missing_values >= 15) & (missing_values <= 70) & (combined_df.dtypes == 'object')].tolist()\n",
    "\n",
    "for column in categorical_cols_high_miss:\n",
    "    combined_df[column] = combined_df.groupby(['District', 'Block'])[column].apply(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else \"Unknown\"))\n",
    "\n",
    "# Check the dataset for any remaining missing values\n",
    "final_missing_values = combined_df.isnull().mean() * 100\n",
    "\n",
    "final_missing_values.sort_values(ascending=False), combined_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Univariate Analysis for Numerical Variables\n",
    "numerical_vars = ['CultLand', 'CropCultLand', 'Acre']\n",
    "categorical_vars = ['District', 'LandPreparationMethod']\n",
    "\n",
    "# Summary statistics for numerical variables\n",
    "summary_statistics = combined_df[numerical_vars].describe()\n",
    "\n",
    "# Creating histograms for numerical variables\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    sns.histplot(combined_df[var], ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Distribution of {var}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Creating box plots for numerical variables\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    sns.boxplot(y=combined_df[var], ax=axes[i])\n",
    "    axes[i].set_title(f'Box Plot of {var}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Univariate Analysis for Categorical Variables\n",
    "# Frequency distribution for categorical variables\n",
    "frequency_distributions = {}\n",
    "for var in categorical_vars:\n",
    "    frequency_distributions[var] = combined_df[var].value_counts()\n",
    "\n",
    "# Creating bar charts for categorical variables\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "for i, var in enumerate(categorical_vars):\n",
    "    sns.barplot(x=frequency_distributions[var].values, y=frequency_distributions[var].index, ax=axes[i])\n",
    "    axes[i].set_title(f'Frequency Distribution of {var}', fontsize=14)\n",
    "    axes[i].set_xlabel('Frequency')\n",
    "    axes[i].set_ylabel(var)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Output the summary statistics and show the plots\n",
    "summary_statistics, plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find outliers using the Interquartile Range (IQR) method\n",
    "def find_outliers_IQR(df, feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[feature] < (Q1 - 1.5 * IQR)) | (df[feature] > (Q3 + 1.5 * IQR))]\n",
    "    return outliers\n",
    "\n",
    "# Investigate outliers for each numerical variable\n",
    "outliers_dict = {}\n",
    "for var in numerical_vars:\n",
    "    outliers_dict[var] = find_outliers_IQR(combined_df, var)\n",
    "\n",
    "# Number of outliers for each variable\n",
    "outliers_count = {var: len(outliers) for var, outliers in outliers_dict.items()}\n",
    "\n",
    "# Proportion of data that is outlier for each variable\n",
    "outliers_proportion = {var: (len(outliers) / len(combined_df)) * 100 for var, outliers in outliers_dict.items()}\n",
    "\n",
    "outliers_count, outliers_proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation to the numerical variables with a small constant to avoid log(0)\n",
    "# Using log1p which is equivalent to log(x+1) to handle zero values\n",
    "for var in numerical_vars:\n",
    "    combined_df[f'log_{var}'] = np.log1p(combined_df[var])\n",
    "\n",
    "# Visualizing the transformed variables\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    sns.histplot(combined_df[f'log_{var}'], ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Log Transformed Distribution of {var}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Creating box plots for the log-transformed numerical variables\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    sns.boxplot(y=combined_df[f'log_{var}'], ax=axes[i])\n",
    "    axes[i].set_title(f'Box Plot of Log Transformed {var}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and drop original columns if their log-transformed versions exist\n",
    "columns_to_drop = []\n",
    "for col in combined_df.columns:\n",
    "    log_col = f'log_{col}'\n",
    "    if log_col in combined_df.columns:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "# Drop the identified columns\n",
    "combined_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique categories for each categorical variable\n",
    "unique_counts = combined_df[categorical_vars].nunique()\n",
    "\n",
    "# One-hot encode 'LandPreparationMethod' as an example\n",
    "# We'll check the number of unique values before deciding to one-hot encode\n",
    "encoded_columns = pd.get_dummies(combined_df['LandPreparationMethod'], prefix='LPM')\n",
    "\n",
    "# Output the unique counts for each categorical variable and the first few rows of the encoded example\n",
    "unique_counts, encoded_columns.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency encoding for 'LandPreparationMethod'\n",
    "frequency_encoding = combined_df['LandPreparationMethod'].value_counts(normalize=True)\n",
    "\n",
    "# Map the frequencies to the original data\n",
    "combined_df['LandPreparationMethod_freq'] = combined_df['LandPreparationMethod'].map(frequency_encoding)\n",
    "\n",
    "# Show the first few entries of the frequency encoded column\n",
    "combined_df[['LandPreparationMethod', 'LandPreparationMethod_freq']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the Train.csv file to extract the 'Yield' feature\n",
    "train_df_yield = pd.read_csv('../Train.csv', usecols=['ID', 'Yield'])\n",
    "\n",
    "# Merge the 'Yield' feature back into the preprocessed dataset using 'ID'\n",
    "combined_df_with_yield = combined_df.merge(train_df_yield, on='ID', how='left')\n",
    "\n",
    "# Separate the entries without 'Yield' data into a new DataFrame\n",
    "df_without_yield = combined_df_with_yield[combined_df_with_yield['Yield'].isnull()]\n",
    "df_with_yield = combined_df_with_yield[combined_df_with_yield['Yield'].notnull()]\n",
    "\n",
    "# Show the first few entries of each dataset to verify the merge\n",
    "df_with_yield.head(), df_without_yield.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed datasets to new csv files for modelling \n",
    "df_with_yield.to_csv('preprocessed_train.csv', index=False)\n",
    "df_without_yield.to_csv('preprocessed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "data_with_yield = pd.read_csv('preprocessed_train.csv')\n",
    "data_without_yield = pd.read_csv('preprocessed_test.csv')\n",
    "\n",
    "# Mark the datasets before combining\n",
    "data_with_yield['is_train'] = True\n",
    "data_without_yield['is_train'] = False\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = pd.concat([data_with_yield, data_without_yield], ignore_index=True)\n",
    "\n",
    "# Apply transformations on the combined dataset\n",
    "\n",
    "# 1. Identifying and Handling Date Features\n",
    "date_cols = []\n",
    "for col in combined_data.columns:\n",
    "    if combined_data[col].dtype == 'object':\n",
    "        # Attempt to convert column to datetime\n",
    "        temp_col = pd.to_datetime(combined_data[col], errors='coerce')\n",
    "        # Check the number of successful conversions\n",
    "        if not temp_col.isna().sum() / len(temp_col) > 0.5:  # Threshold for successful conversion\n",
    "            date_cols.append(col)\n",
    "            combined_data[col] = temp_col\n",
    "            # Extracting date components\n",
    "            combined_data[f'{col}_day'] = combined_data[col].dt.day\n",
    "            combined_data[f'{col}_month'] = combined_data[col].dt.month\n",
    "            combined_data[f'{col}_year'] = combined_data[col].dt.year\n",
    "            # Calculate elapsed time\n",
    "            reference_date = max(combined_data[col].max(), pd.Timestamp('now'))\n",
    "            combined_data[f'DaysSince_{col}'] = (reference_date - combined_data[col]).dt.days\n",
    "\n",
    "\n",
    "# 2. Transforming High Cardinality Categorical Features\n",
    "high_cardinality_cols = ['LandPreparationMethod', 'SeedingSowingTransplanting', 'NursDetFactor', 'TransDetFactor']\n",
    "\n",
    "def split_methods(df, column):\n",
    "    if df[column].dtype == object:\n",
    "        methods_series = df[column].str.split(' ')\n",
    "        unique_methods = set(method for method_list in methods_series.dropna() for method in method_list)\n",
    "        for method in unique_methods:\n",
    "            df[f'{column}_{method}'] = methods_series.apply(lambda x: method in x if x is not None else False)\n",
    "    return df.drop(columns=[column], errors='ignore')\n",
    "\n",
    "# Applying the transformation to high cardinality columns\n",
    "for col in high_cardinality_cols:\n",
    "    combined_data = split_methods(combined_data, col)\n",
    "\n",
    "\n",
    "# 3. One-hot Encoding for Moderate and Low Cardinality Categorical Features\n",
    "categorical_cols = combined_data.select_dtypes(include=['object']).columns\n",
    "categorical_cols = categorical_cols.drop('ID')  # Exclude 'ID' from one-hot encoding\n",
    "combined_data = pd.get_dummies(combined_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 4. Normalizing Skewed Numerical Features (apply before separating, excluding 'Yield')\n",
    "numerical_cols = combined_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "numerical_cols = numerical_cols.drop(['Yield', 'ID'], errors='ignore') # Exclude 'Yield' and 'ID' from this transformation\n",
    "numerical_skewness = combined_data[numerical_cols].skew()\n",
    "highly_skewed_features = numerical_skewness[abs(numerical_skewness) > 1].index\n",
    "\n",
    "for feature in highly_skewed_features:\n",
    "    combined_data[f'log_{feature}'] = np.log(combined_data[feature] + 1)\n",
    "\n",
    "# Splitting the datasets\n",
    "data_with_yield = combined_data[combined_data['is_train']]\n",
    "data_without_yield = combined_data[~combined_data['is_train']]\n",
    "data_with_yield.drop('is_train', axis=1, inplace=True)\n",
    "data_without_yield.drop('is_train', axis=1, inplace=True)\n",
    "\n",
    "# Applying log transformation to the 'Yield' column\n",
    "if 'Yield' in data_with_yield.columns:\n",
    "    data_with_yield['log_Yield'] = np.log(data_with_yield['Yield'] + 1)\n",
    "\n",
    "# Display the first few rows of the transformed datasets\n",
    "print(data_with_yield.head())\n",
    "print(data_without_yield.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from kerastuner.tuners import RandomSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'ID' column from data_with_yield \n",
    "data_with_yield = data_with_yield.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CropTillageDate</th>\n",
       "      <th>CropTillageDepth</th>\n",
       "      <th>RcNursEstDate</th>\n",
       "      <th>SeedlingsPerPit</th>\n",
       "      <th>TransplantingIrrigationHours</th>\n",
       "      <th>TransIrriCost</th>\n",
       "      <th>StandingWater</th>\n",
       "      <th>Ganaura</th>\n",
       "      <th>CropOrgFYM</th>\n",
       "      <th>...</th>\n",
       "      <th>log_Harv_hand_rent</th>\n",
       "      <th>log_Residue_length</th>\n",
       "      <th>log_Residue_perc</th>\n",
       "      <th>log_log_Acre</th>\n",
       "      <th>log_Harv_date_month</th>\n",
       "      <th>log_Harv_date_year</th>\n",
       "      <th>log_DaysSince_Harv_date</th>\n",
       "      <th>log_Threshing_date_month</th>\n",
       "      <th>log_Threshing_date_year</th>\n",
       "      <th>log_Yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_GTFAC7PEVWQ9</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>...</td>\n",
       "      <td>6.324520</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>5.891644</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>6.398595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_TK40ARLSPOKS</td>\n",
       "      <td>2022-07-18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>125.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>22.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>5.866468</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>6.398595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1FJY2CRIMLZZ</td>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.175867</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.129404</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>5.817111</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>7.612831</td>\n",
       "      <td>5.420535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_I3IPXS4DB7NE</td>\n",
       "      <td>2022-06-16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2022-06-17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.100613</td>\n",
       "      <td>216.0</td>\n",
       "      <td>3.252529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>...</td>\n",
       "      <td>5.484797</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.182880</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>5.846439</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>6.150603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_4T8YQWXWHB4A</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-06-21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.324520</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>0.325275</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>5.852202</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>7.612337</td>\n",
       "      <td>6.311735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID CropTillageDate  CropTillageDepth RcNursEstDate  \\\n",
       "0  ID_GTFAC7PEVWQ9      2022-07-20               5.0    2022-06-27   \n",
       "1  ID_TK40ARLSPOKS      2022-07-18               5.0    2022-06-20   \n",
       "2  ID_1FJY2CRIMLZZ      2022-06-30               6.0    2022-06-20   \n",
       "3  ID_I3IPXS4DB7NE      2022-06-16               6.0    2022-06-17   \n",
       "4  ID_4T8YQWXWHB4A      2022-07-19               4.0    2022-06-21   \n",
       "\n",
       "   SeedlingsPerPit  TransplantingIrrigationHours  TransIrriCost  \\\n",
       "0              2.0                      5.000000          200.0   \n",
       "1              2.0                      5.000000          125.0   \n",
       "2              2.0                      4.000000           80.0   \n",
       "3              2.0                      8.100613          216.0   \n",
       "4              2.0                      9.000000          300.0   \n",
       "\n",
       "   StandingWater  Ganaura  CropOrgFYM  ...  log_Harv_hand_rent  \\\n",
       "0       2.000000     21.0         3.6  ...            6.324520   \n",
       "1       3.000000     22.8         8.8  ...            1.386294   \n",
       "2       2.000000      1.0         1.0  ...            6.175867   \n",
       "3       3.252529      1.0         1.2  ...            5.484797   \n",
       "4       2.000000     21.2         2.0  ...            6.324520   \n",
       "\n",
       "   log_Residue_length  log_Residue_perc  log_log_Acre  log_Harv_date_month  \\\n",
       "0            3.433987          3.713572      0.240538             2.484907   \n",
       "1            3.218876          2.397895      0.240538             2.484907   \n",
       "2            3.433987          2.397895      0.129404             2.564949   \n",
       "3            3.295837          2.397895      0.182880             2.564949   \n",
       "4            3.218876          3.713572      0.325275             2.484907   \n",
       "\n",
       "   log_Harv_date_year  log_DaysSince_Harv_date log_Threshing_date_month  \\\n",
       "0            7.612337                 5.891644                 2.484907   \n",
       "1            7.612337                 5.866468                 2.564949   \n",
       "2            7.612337                 5.817111                 0.693147   \n",
       "3            7.612337                 5.846439                 2.564949   \n",
       "4            7.612337                 5.852202                 2.564949   \n",
       "\n",
       "   log_Threshing_date_year log_Yield  \n",
       "0                 7.612337  6.398595  \n",
       "1                 7.612337  6.398595  \n",
       "2                 7.612831  5.420535  \n",
       "3                 7.612337  6.150603  \n",
       "4                 7.612337  6.311735  \n",
       "\n",
       "[5 rows x 209 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_yield.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and drop original columns if their log-transformed versions exist\n",
    "columns_to_drop = []\n",
    "for col in data_with_yield.columns:\n",
    "    log_col = f'log_{col}'\n",
    "    if log_col in data_with_yield.columns:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "# Drop the identified columns\n",
    "data_with_yield.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "data_with_yield.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_yield.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and drop original columns if their log-transformed versions exist\n",
    "columns_to_drop = []\n",
    "for col in data_without_yield.columns:\n",
    "    log_col = f'log_{col}'\n",
    "    if log_col in data_without_yield.columns:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "# Drop the identified columns\n",
    "data_without_yield.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "data_without_yield.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed datasets to new csv files for modelling\n",
    "data_with_yield.to_csv('preprocessed+_train.csv', index=False)\n",
    "data_without_yield.to_csv('preprocessed+_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that only numerical columns are used for scaling\n",
    "numerical_cols = data_with_yield.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Filter out non-numerical columns\n",
    "X_numerical = data_with_yield[numerical_cols]\n",
    "\n",
    "# Separate features and target variable\n",
    "X = X_numerical.drop('log_Yield', axis=1)\n",
    "y = data_with_yield['log_Yield'].values\n",
    "\n",
    "# Normalizing the data - only apply scaler to numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model building function for tuning\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int('input_units', min_value=64, max_value=512, step=64), activation='relu', input_shape=(X_train_full.shape[1],)))\n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 5)):\n",
    "        model.add(Dense(hp.Int(f'dense_{i}_units', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}_rate', min_value=0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import Objective\n",
    "\n",
    "# Initialize and configure the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=Objective('val_loss', direction='min'),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='model_tuning',\n",
    "    project_name='yield_prediction'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 33s]\n",
      "val_loss: 0.13474103808403015\n",
      "\n",
      "Best val_loss So Far: 0.08985296885172527\n",
      "Total elapsed time: 00h 03m 37s\n"
     ]
    }
   ],
   "source": [
    "# Start the hyperparameter tuning process\n",
    "tuner.search(X_train_full, y_train_full, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Define the K-Fold cross-validator\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize list to store RMSE for each fold\n",
    "rmse_scores = []\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "for train_index, val_index in kf.split(X_train_full):\n",
    "    # Split data\n",
    "    X_train, X_val = X_train_full[train_index], X_train_full[val_index]\n",
    "    y_train, y_val = y_train_full[train_index], y_train_full[val_index]\n",
    "\n",
    "    # Build the model with the best hyperparameters\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=5)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    rmse_scores.append(val_rmse)\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(\"Average RMSE across all folds:\", average_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the 'Yield' column is not in the dataset before scaling\n",
    "if 'Yield' in data_without_yield.columns:\n",
    "    data_without_yield = data_without_yield.drop(columns=['Yield'])\n",
    "\n",
    "# Select only numerical columns for prediction\n",
    "data_without_yield_numerical = data_without_yield.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Normalize the data - only apply scaler to numerical columns\n",
    "data_without_yield_numerical = scaler.transform(data_without_yield_numerical)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predicted_yield = model.predict(data_without_yield_numerical)\n",
    "\n",
    "# Convert predictions to the original scale if they were log-transformed\n",
    "predicted_yield = np.exp(predicted_yield) - 1  # If 'log_Yield' was used as target\n",
    "\n",
    "# Ensure predicted 'Yield' is of type float\n",
    "predicted_yield = predicted_yield.flatten().astype(float)\n",
    "\n",
    "# Create a DataFrame with IDs and corresponding predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Yield': predicted_yield.flatten()  # Flatten to convert from 2D to 1D array\n",
    "})\n",
    "\n",
    "# Restricting 'Yield' to two decimal places\n",
    "predictions_df['Yield'] = predictions_df['Yield'].round(2)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('subtk7.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the predictions DataFrame\n",
    "predictions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming data_with_yield is a pandas DataFrame with the target column 'log_Yield'\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = data_with_yield.drop(columns=['log_Yield'])  # Drop 'ID' if it's not a feature\n",
    "y = data_with_yield['log_Yield']\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostRegressor\n",
    "catboost_model = cb.CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='RMSE',\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "catboost_model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "\n",
    "# Save the model to a file\n",
    "catboost_model.save_model('catboost_model.cbm')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = catboost_model.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, predictions))\n",
    "print(\"Validation RMSE:\", val_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data_without_yield dataset by selecting the same features used for training\n",
    "# Exclude the 'ID' column if it's not used as a feature\n",
    "X_to_predict = data_without_yield.drop(columns=['ID'])\n",
    "\n",
    "# Make predictions using the CatBoost model\n",
    "predictions_without_yield = catboost_model.predict(X_to_predict)\n",
    "\n",
    "# Reverse the log transformation if you used log1p during training\n",
    "predictions_without_yield = np.expm1(predictions_without_yield)\n",
    "\n",
    "# Round predictions to two decimal places\n",
    "predictions_without_yield = np.round(predictions_without_yield, 2)\n",
    "\n",
    "# Create a DataFrame with IDs and corresponding predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ID': data_without_yield['ID'],  # Make sure 'ID' column exists in the data_without_yield\n",
    "    'Yield': predictions_without_yield\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('catboost_tk7.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the predictions DataFrame\n",
    "predictions_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
